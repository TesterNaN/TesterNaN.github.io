<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TesterNaN的博客</title>
  
  
  <link href="https://testernan.github.io/atom.xml" rel="self"/>
  
  <link href="https://testernan.github.io/"/>
  <updated>2023-03-06T08:19:53.501Z</updated>
  <id>https://testernan.github.io/</id>
  
  <author>
    <name>TesterNaN</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用stable-diffusion-webui部署NovelAi/Stable Diffusion 保姆级教程、命令解释、原理讲解(colab、windows、Linux )</title>
    <link href="https://testernan.github.io/2023/03/06/%E4%BD%BF%E7%94%A8stable-diffusion-webui%E9%83%A8%E7%BD%B2NovelAi-Stable-Diffusion-%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%95%99%E7%A8%8B%E3%80%81%E5%91%BD%E4%BB%A4%E8%A7%A3%E9%87%8A%E3%80%81%E5%8E%9F%E7%90%86%E8%AE%B2%E8%A7%A3-colab%E3%80%81windows%E3%80%81Linux/"/>
    <id>https://testernan.github.io/2023/03/06/%E4%BD%BF%E7%94%A8stable-diffusion-webui%E9%83%A8%E7%BD%B2NovelAi-Stable-Diffusion-%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%95%99%E7%A8%8B%E3%80%81%E5%91%BD%E4%BB%A4%E8%A7%A3%E9%87%8A%E3%80%81%E5%8E%9F%E7%90%86%E8%AE%B2%E8%A7%A3-colab%E3%80%81windows%E3%80%81Linux/</id>
    <published>2023-03-06T08:13:05.000Z</published>
    <updated>2023-03-06T08:19:53.501Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近NovalAI的部署教程随处可见,可基本都是windows本地和colab白嫖google羊毛的、更多面向使用者 想要自己在Linux服务器上部署、找了一圈都没满意的、官方的一键脚本没有考虑国内网络的环境、很多大佬也不屑于写诸如此类的东西、导致踩了无数的坑才成功部署 希望借此文来帮助和我一样想部署在Linux服务器上的同学们,也好对AI黑盒里的技术窥视一番  </p></blockquote><ul><li><p>正文开始前先讲一讲、目前为止SD的部署方案汇总</p></li><li><p>colab(科学上网 谷歌账号 包含两种ui界面的colab)</p></li><li><p><a href="https://colab.research.google.com/drive/1_Ma71L6uGbtt6UQyA3FjqW2lcZ5Bjck-">原版ui+naifu</a></p></li><li><p><a href="https://colab.research.google.com/drive/1Y5WX9F69xibL6sJ9crgnm3BgPMpikAsT#scrollTo=ZzRNMT42Gw_p">sd-webui+novelai</a></p></li><li><p><a href="https://breezy-andesaurus-8f0.notion.site/colab-2f1f5b4815e74e34a9add78afdab83cd">个人整理notion简陋教程</a></p></li><li><p>windows(本地)</p></li><li><p><a href="https://b23.tv/q5nTEEG">b站教程</a></p></li><li><p><strong>Linux+SDWebUi 本文内容</strong></p></li><li><p><strong>利用autodl平台自带的公网ip映射服务( 强烈推荐,没有有效时间限制,访问速度快)</strong></p></li><li><p><strong>gradio</strong> (负责自动化生成部署生成一个网页,72小时后失效)</p></li><li><p>Linux 前后端分离 docker nginx postgresql (需要自己的公网ip或域名 显存 内存 要求最高)</p></li></ul><p>10.29号更新:优雅的远程开发工作流</p><h2 id="正文部分-Linux-SDWebUi-部署NovelAi"><a href="#正文部分-Linux-SDWebUi-部署NovelAi" class="headerlink" title="正文部分:Linux+SDWebUi 部署NovelAi"></a>正文部分:Linux+SDWebUi 部署NovelAi</h2><ul><li><p><strong>用到的工具</strong></p></li><li><p>aria2 (类似于curl的下载软件 用于后面下载模型文件)</p></li><li><p><strong>自行安装的依赖(就是把依赖中的硬骨头先一个个手动安装 别的借助requirement.txt自动安装)</strong></p></li><li><p>pytorch cuda</p></li><li><p><strong>11.3号更新 gradio3.8已经可以用pip正常下载</strong></p></li><li><p><strong>用到的开源组件</strong></p></li><li><p>stable-diffusion(sd本体、webUI就是封装了个UI(当然还集成了一众优秀的功能)让我们能通过可视化界面而不是通过命令行参数使用SD绘画创作)</p></li><li><p>BLIP (interrogate CLIP的依赖 负责img2img中描述input图像内容并输入至prompt框)</p></li><li><p>taming-transformers (stablediffusion的高分辨率图像的生成)</p></li><li><p>k-diffusion(为SD提供samplers(采样器)SDE(随机微分方程)和ODE(常微分方程))</p></li><li><p>midas(负责为sd 深度模型提供支持)</p></li></ul><p>更多详情请见</p><h3 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h3><h2 id="TLDR-2023-2-3更新"><a href="#TLDR-2023-2-3更新" class="headerlink" title="TLDR(2023.2.3更新)"></a>TLDR(2023.2.3更新)</h2><p><strong>制作镜像封装了最新的webUI,内置所有依赖所需的权重文件(考虑到webUI在更换依赖到pytorch 1 .13. 1后性能方面提升显著,也将迎来稳定期,所以做一个全面稳定的镜像方便自己和大家使用),内置anything4.5模型、常用vae权重文件、以及一些常用插件</strong></p><p><strong>无需任何编程基础</strong> <a href="https://www.autodl.com/register?code=6dd7e133-fd0f-432a-ad52-d683ada06582">注册autodl账号</a> <strong>后直接照着下面视频使用教程操作即可完成部署使用的全过程</strong></p><p><a href="https://www.codewithgpu.com/i/AUTOMATIC1111/stable-diffusion-webui/stablediffusionwebui">镜像链接</a></p><p>使用教程</p><hr><h2 id="选用autodl平台的服务器-别的服务器平台基本一致"><a href="#选用autodl平台的服务器-别的服务器平台基本一致" class="headerlink" title="选用autodl平台的服务器(别的服务器平台基本一致)"></a><strong>选用autodl平台的服务器(别的服务器平台基本一致)</strong></h2><ul><li><p>注册 <a href="https://www.autodl.com/register?code=6dd7e133-fd0f-432a-ad52-d683ada06582">autodl注册界面(新用户注册会送十块钱)</a></p></li><li><p>选实例 参照colab谷歌给我们白嫖的机器配置 (autodl 性价比推荐 内蒙古 A5000 24G显存 )</p></li><li><p>选镜像 直接miniconda python3.8 cuda11.3</p></li><li><p>(2023.1.25 更新 已封装了webUI全套镜像内置所有依赖所需的权重文件 内置anything4.5 复制镜像创建实例后 直接在webUI目录下执行启动命令即可启动镜像cuda11.7 pytorch 1.13.0)</p></li><li><p>点击我的实例-快捷工具-jupyterLab 启动页点击终端新建一个终端</p></li></ul><h2 id="下载安装依赖"><a href="#下载安装依赖" class="headerlink" title="下载安装依赖"></a><strong>下载安装依赖</strong></h2><ul><li><p>进入数据盘路径 <code>cd autodl-tmp</code></p></li><li><p><a href="https://www.autodl.com/docs/network_turbo/">加速github下载速度</a>(找到自己的区并在终端输入相应的语句 )</p></li><li><p>更新下apt下载工具</p></li><li><p><code>apt update</code></p></li><li><p>补充安装python3环境</p></li><li><p><code>sudo apt install wget git python3 python3-venv</code></p></li><li><p>用pip安装 pytorch 和cuda (服务器conda有bug pip默认就加过速了 所以直接用pip下载安装)</p></li><li><p><code>pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113</code></p></li><li><p>检查cuda和pytorch是否可用</p></li><li><p><code>python -c &quot;import torch; print(torch.cuda.is_available())&quot;</code></p></li><li><p>下载aria2 类似于curl 下载软件 用于后面下载gradio和模型文件</p></li><li><p><code>apt install aria2</code></p></li><li><p><strong>11</strong> <strong>.3 号更新 gradio更新到 3.8 版本 现在可以直接通过pip 从远处包仓库中下载安装 pip install gradio&#x3D;&#x3D;3.8即可</strong></p></li><li><p>gradio (负责自动化生成部署生成一个网页(前端和中间件去代理绘图程序API),使你能通过http和前端界面与后端API交互)</p></li><li><p>11 .27 号更新 webui又加了一堆依赖… 其中torchsde会出现pip下载过程镜像源中找不到对应包的问题 torchsde是webui近期更新后增加的依赖 可以去pypi 上搜torchsde 然后将安装包whl文件下到机子上 然后pip install 加刚才的whl文件名 手动安装</p></li></ul><h3 id="下载Stable-Diffusion-Webui框架和SD绘画各个步骤用到的模型框架"><a href="#下载Stable-Diffusion-Webui框架和SD绘画各个步骤用到的模型框架" class="headerlink" title="下载Stable-Diffusion-Webui框架和SD绘画各个步骤用到的模型框架"></a><strong>下载Stable-Diffusion-Webui框架和SD绘画各个步骤用到的模型框架</strong></h3><p>Stable Diffusion WebUi简称 SDWebUi，web UI是一个基于 Gradio 库的 Stable Diffusion 浏览器界面。</p><ul><li><p>下载 web ui 框架并进入路径</p></li><li><p><code>git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git</code></p></li><li><p><code>cd stable-diffusion-webui</code></p></li><li><p>下载 StableDiffusion(绘图) 和 CodeFormr(脸部修复)</p></li><li><p><code>mkdir repositories</code></p></li><li><p><code>git clone https://github.com/CompVis/stable-diffusion.git repositories/stable-diffusion</code></p></li><li><p><code>git clone https://github.com/CompVis/taming-transformers.git repositories/taming-transformers</code></p></li><li><p><code>git clone https://github.com/sczhou/CodeFormer.git repositories/CodeFormer</code></p></li><li><p><code>git clone https://github.com/salesforce/BLIP.git repositories/BLIP</code></p></li><li><p>下载安装Stable Diffusion的依赖</p></li><li><p><code>pip install transformers==4.19.2 diffusers invisible-watermark --prefer-binary</code></p></li><li><p>下载安装k-diffusion</p></li><li><p><code>pip install git+https://github.com/crowsonkb/k-diffusion.git --prefer-binary</code></p></li><li><p>下载gfpgan 可选 负责脸部修复face restoration</p></li><li><p><code>pip install gfpgan</code></p></li><li><p>安装之前下载的CodeFormer的依赖</p></li><li><p><code>pip install -r repositories/CodeFormer/requirements.txt --prefer-binary</code></p></li><li><p>安装 web ui框架的依赖</p></li><li><p><code>pip install -r requirements.txt --prefer-binary</code></p></li><li><p>更新numpy到最新版本(因为之前很多包都会引用到<code>numpy</code>所以为了保证版本统一,在此更新为最新版)</p></li><li><p><code>pip install -U numpy --prefer-binary</code></p></li></ul><h2 id="下载模型文件"><a href="#下载模型文件" class="headerlink" title="下载模型文件"></a><strong>下载模型文件</strong></h2><p>如需别的模型请自行替换(也可以多个并存,根据需要在画图过程中在页面左上角切换) 位置一律放在&#x2F;stable-diffusion-webui&#x2F;models&#x2F;Stable-diffusion&#x2F; 下</p><p>10.19号成功解决模型切换的问题后、现在可以控制相同prompt seed,快速对照SDv1.4和NovelAI生成效果.</p><p><img src="https://pic1.zhimg.com/v2-d1a5c604913f3fc4da39dba5804bc1f4_r.jpg"></p><p>Stable Diffusion1.4与NovelAI对照效果</p><h3 id="NovelAI-10-20号更新-因为链接都有期限-大家最好自己备份下-更多模型可见"><a href="#NovelAI-10-20号更新-因为链接都有期限-大家最好自己备份下-更多模型可见" class="headerlink" title="NovelAI(10.20号更新 因为链接都有期限 大家最好自己备份下 ) 更多模型可见"></a>NovelAI(10.20号更新 因为链接都有期限 大家最好自己备份下 ) 更多模型可见</h3><p><a href="https://rentry.org/sdmodels#">https://rentry.org/sdmodels#</a></p><pre><code class="bash">aria2c --summary-interval=10 -d ./models/Stable-diffusion/ -x 3 --allow-overwrite=true https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/animefull-latest.tar</code></pre><p>因为这个链接下的是tar压缩包 所以需要tar xf 解压 并rm 删除压缩包 (具体命令自行网上查阅)<strong>注意</strong>: 最后ckpt一定要在 <strong>&#x2F;models&#x2F;Stable-diffusion&#x2F;目录下</strong></p><ul><li>tar xf animefull-latest.tar</li><li>rm animefull-latest.tar</li></ul><h3 id="Stable-diffusion-1-4"><a href="#Stable-diffusion-1-4" class="headerlink" title="Stable-diffusion 1.4"></a>Stable-diffusion 1.4</h3><p>去HuggingFace去下载(注册账号 同意条款 然后就能看到下载链接)</p><p><a href="https://huggingface.co/CompVis/stable-diffusion-v-1-4-original">CompVis&#x2F;stable-diffusion-v-1-4-original · Hugging Face</a></p><h3 id="10-23日更新-Stable-diffusion-1-5"><a href="#10-23日更新-Stable-diffusion-1-5" class="headerlink" title="(10.23日更新)Stable-diffusion 1.5"></a>(10.23日更新)Stable-diffusion 1.5</h3><p><a href="https://huggingface.co/runwayml/stable-diffusion-v1-5">runwayml&#x2F;stable-diffusion-v1-5 · Hugging Face</a></p><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a><strong>运行</strong></h2><p>下面是运行程序的两种方法 <strong>(10.18日更新 :添加一种更好的方法,即利用autodl平台的公网ip映射服务来实现部署服务,无需借助gradio,强烈推荐)</strong></p><hr><h3 id="方法一-利用autodl平台自带的公网ip映射服务-没有有效时间限制-访问速度快"><a href="#方法一-利用autodl平台自带的公网ip映射服务-没有有效时间限制-访问速度快" class="headerlink" title="方法一:利用autodl平台自带的公网ip映射服务(没有有效时间限制,访问速度快)"></a>方法一:利用autodl平台自带的公网ip映射服务(没有有效时间限制,访问速度快)</h3><p><strong>10.28号更新、为防止该服务被滥用、现在在autodl平台使用该服务需先实名制、所以使用时请合法合规</strong></p><p>AutoDL为每个实例都预留了一个可对外暴露的端口，技术实现为将实例中的<code>6006</code>端口映射到公网可供访问的<code>ip:port</code>上,如果您自启动的服务带有Web网页那么点击访问后就可以打开Web网页。如果您启动的是API服务，那么通过打开的地址进行API调用即可（因为没有Web网页，打开后不会有任何网页显示）</p><ul><li>1 在终端中输入运行下列命令–port 6006将程序部署在服务器本机的6006端口</li></ul><pre><code class="bash">COMMANDLINE_ARGS=&quot;--medvram --always-batch-cond-uncond --port 6006&quot; REQS_FILE=&quot;requirements.txt&quot; python launch.py</code></pre><ul><li>2 找到控制台界面,找到你实例的自定义服务按钮,点击访问即可进入autodl平台帮我们映射到公网的网站(之后可以从其他设备的浏览器访问这个网址来使用部署的绘画服务)</li></ul><p><img src="https://pic3.zhimg.com/v2-cf2a56fad275dd49a0eaef0930277dca_b.jpg"></p><ul><li>如需了解更多,可在autodl帮助文档的这个子页面中找到 <a href="https://www.autodl.com/docs/port/">https://www.autodl.com/docs/port/</a></li></ul><h3 id="方法二-SSH端口转发-ssh隧道-10-28号autodl-公网映射监管收紧后-推荐该方法-安全系数最高"><a href="#方法二-SSH端口转发-ssh隧道-10-28号autodl-公网映射监管收紧后-推荐该方法-安全系数最高" class="headerlink" title="方法二:SSH端口转发(ssh隧道) 10.28号autodl 公网映射监管收紧后 推荐该方法 安全系数最高"></a>方法二:SSH端口转发(ssh隧道) 10.28号autodl 公网映射监管收紧后 推荐该方法 安全系数最高</h3><p>访问一个远程机器上没有公开的端口。这种情况下可以使用SSH隧道技术(或者叫端口转发)将所需的远程端口 “转发 “到你的本地机器,然后就可以通过访问自己机器的本地端口来使用远程程序的相应端口上的服务</p><p>限于篇幅控制、具体操作见下文中的具体演示</p><p><img src="https://pic1.zhimg.com/v2-89575d9803519dba7b28b276fc790ab0_b.jpg"></p><p>演示</p><h3 id="方法三-利用gradio提供部署服务-每次可使用72小时-相较于第一种方法访问速度会慢些-安全性差"><a href="#方法三-利用gradio提供部署服务-每次可使用72小时-相较于第一种方法访问速度会慢些-安全性差" class="headerlink" title="方法三:利用gradio提供部署服务(每次可使用72小时,相较于第一种方法访问速度会慢些,安全性差)"></a>方法三:利用gradio提供部署服务(每次可使用72小时,相较于第一种方法访问速度会慢些,安全性差)</h3><p>如果你用的不是autodl平台,别的平台估计也有类似的公网ip映射服务 (我猜的2333) 如果没有而又想要通过URL给他人使用演示,就只能借助gradio这种方法了</p><ol><li>在终端中输入运行下列命令</li></ol><p><code>COMMANDLINE_ARGS=&quot;--medvram --always-batch-cond-uncond --share&quot; REQS_FILE=&quot;requirements.txt&quot; python launch.py</code></p><p>--share参数 会得到一个以.app.gradio 结尾的链接，这是在协作中使用该程序的预期方式。(不加 –share 没法远程使用)</p><p>2. 访问上面的链接即可开始使用</p><h3 id="运行程序的一些命令行参数及解释"><a href="#运行程序的一些命令行参数及解释" class="headerlink" title="运行程序的一些命令行参数及解释"></a>运行程序的一些命令行参数及解释</h3><table><thead><tr><th>命令行参数</th><th>解释</th></tr></thead><tbody><tr><td>--xformers</td><td>使用xformers库。极大地改善了内存消耗和速度。Windows 版本安装由C43H66N12O12S2 维护的二进制文件</td></tr><tr><td>--force-enable-xformers</td><td>无论程序是否认为您可以运行它，都启用 xformers。不要报告你运行它的错误。</td></tr><tr><td>--opt-split-attention</td><td>Cross attention layer optimization 优化显着减少了内存使用，几乎没有成本（一些报告改进了性能）。黑魔法。默认情况下torch.cuda，包括 NVidia 和 AMD 卡。</td></tr><tr><td>--disable-opt-split-attention</td><td>禁用上面的优化</td></tr><tr><td>--opt-split-attention-v1</td><td>使用上述优化的旧版本，它不会占用大量内存（它将使用更少的 VRAM，但会限制您可以制作的最大图片大小）。</td></tr><tr><td>--medvram</td><td>通过将稳定扩散模型分为三部分，使其消耗更少的VRAM，即cond（用于将文本转换为数字表示）、first_stage（用于将图片转换为潜在空间并返回）和unet（用于潜在空间的实际去噪），并使其始终只有一个在VRAM中，将其他部分发送到CPU RAM。降低性能，但只会降低一点-除非启用实时预览。</td></tr><tr><td>--lowvram</td><td>对上面更彻底的优化，将 unet 拆分成多个模块，VRAM 中只保留一个模块,破坏性能</td></tr><tr><td>*do-not-batch-cond-uncond</td><td>防止在采样过程中对正面和负面提示进行批处理，这基本上可以让您以 0.5 批量大小运行，从而节省大量内存。降低性能。不是命令行选项，而是使用–medvramor 隐式启用的优化–lowvram。</td></tr><tr><td>--always-batch-cond-uncond</td><td>禁用上述优化。只有与–medvram或–lowvram一起使用才有意义</td></tr><tr><td>--opt-channelslast</td><td>更改 torch 内存类型，以稳定扩散到最后一个通道,效果没有仔细研究。</td></tr></tbody></table><h2 id="Q-amp-A-自己碰到的bug-欢迎评论区添加"><a href="#Q-amp-A-自己碰到的bug-欢迎评论区添加" class="headerlink" title="Q&amp;A(自己碰到的bug 欢迎评论区添加)"></a><strong>Q&amp;A(自己碰到的bug 欢迎评论区添加)</strong></h2><ul><li><p>Q：torchsde报错找不到合适版本</p></li><li><p>A：torchsde是webui近期更新后增加的依赖 可以去pypi 上搜torchsde 然后将安装包whl文件下到机子上 然后pip install 加刚才的whl文件名 手动安装</p></li><li><p>Q:系统环境混乱 重置</p></li><li><p>A:关机更换镜像 再开机</p></li><li><p>Q:报错未知参数 scale</p></li><li><p>A:gradio更新到最新3.5</p></li><li><p>Q:出现 <code>ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed.</code>报错</p></li><li><p>A:不用管,倒数第三步解决该问题</p></li><li><p>Q:换别的模型报错(<strong>10.19号更新)</strong></p></li><li><p>A:10.19号更新 确定为之前版本Stable Diffusion Webui的bug <strong>在Stable Diffusion Webui 路径下 执行 git pull 命令拉取最新版webui 即可解决(估计是10.18号的官方更新代码中修复的)</strong></p></li><li><p>Q:每次重启机器后如何再次运行程序</p></li><li><p>A:cd 到webui路径下,输入最后那串命令(github加速 export 环境变量在当前终端关闭后会失效 所以重启后需再次执行)</p></li><li><p>Q:autodl系统盘占用特别多,不知道怎么回事就满了</p></li><li><p>A:</p></li><li><p>Q:export 环境变量加速github下载后,本地端口转发 gradio安全性报错</p></li></ul><p><img src="https://pic3.zhimg.com/v2-e627c7bc7090443b4675c024193e0a9e_b.png"></p><ul><li>A:到gradio源码里把检查判定逻辑那几行代码注释掉(每次git pull 更新webui前记得把注释取消)</li></ul><p><img src="https://pic3.zhimg.com/v2-b7b9f815d3743c5e30586f3e3d9de236_b.jpg"></p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h2><p><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki">SDwebui官方文档</a></p><p><a href="https://www.autodl.com/docs/deps/">autodl帮助文档</a></p><p><a href="https://stable-diffusion-book.vercel.app/">stable-diffusion-book</a></p><hr><p>ps: 是的,封面就是用部署完的服务器画的 ╰( ’ ’ )╮</p><h2 id="更新动态-后续会按技术原理部署、使用训练、新闻动态等分类整理重构下-便于大家阅读索引"><a href="#更新动态-后续会按技术原理部署、使用训练、新闻动态等分类整理重构下-便于大家阅读索引" class="headerlink" title="更新动态(后续会按技术原理部署、使用训练、新闻动态等分类整理重构下,便于大家阅读索引)"></a>更新动态(后续会按技术原理部署、使用训练、新闻动态等分类整理重构下,便于大家阅读索引)</h2><h3 id="2023-1-25-最新的webUI-又更新了环境需求-要pytorch-1-13-1-pip镜像源里尚无-需要自行去pypi下载"><a href="#2023-1-25-最新的webUI-又更新了环境需求-要pytorch-1-13-1-pip镜像源里尚无-需要自行去pypi下载" class="headerlink" title="2023.1.25 最新的webUI 又更新了环境需求 要pytorch 1.13.1 pip镜像源里尚无 需要自行去pypi下载"></a>2023.1.25 最新的webUI 又更新了环境需求 要pytorch 1.13.1 pip镜像源里尚无 需要自行去pypi下载</h3><p>为方便大家更新 已全套打包上传<a href="https://pan.baidu.com/s/1fMoGBYG3_LND5A3s6S9WKQ?pwd=kw2j">百度云</a> cp38 linux 和我环境一致可以直接下载安装</p><ul><li>先卸载 这三个</li><li>然后在pip install 加文件名 本地安装</li></ul><p><img src="https://pic3.zhimg.com/v2-0478db1797e9e10ea1314fd5abc3aaf6_b.jpg"></p><h3 id="2023-1-9-新年第一篇-讲讲如何找到心仪的模型"><a href="#2023-1-9-新年第一篇-讲讲如何找到心仪的模型" class="headerlink" title="2023.1.9 新年第一篇,讲讲如何找到心仪的模型"></a>2023.1.9 新年第一篇,讲讲如何找到心仪的模型</h3><h3 id="11-27-stable-diffusion-2-0近日已发布-因为使用方法与之前略有不同-所以介绍一下"><a href="#11-27-stable-diffusion-2-0近日已发布-因为使用方法与之前略有不同-所以介绍一下" class="headerlink" title="11.27 stable diffusion 2.0近日已发布 因为使用方法与之前略有不同 所以介绍一下"></a>11.27 stable diffusion 2.0近日已发布 因为使用方法与之前略有不同 所以介绍一下</h3><ol><li>下载模型文件 768-v-ema.ckpt</li></ol><p><a href="https://huggingface.co/stabilityai/stable-diffusion-2/tree/main">stabilityai&#x2F;stable-diffusion-2 at main</a></p><p><img src="https://pic2.zhimg.com/v2-56853fa21324bc8bccbcfc3a82539325_b.jpg"></p><p>2.把ckpt文件放入 models&#x2F;Stable-Diffusion 路径下</p><p>3.下载SD2.0模型配置文件 <a href="https://github.com/Stability-AI/stablediffusion/tree/main/configs/stable-diffusion">stablediffusion&#x2F;configs&#x2F;stable-diffusion at main · Stability-AI&#x2F;stablediffusion</a> 即下载下图中的v2-inference-v.yaml，并将其放在与检查点相同的地方(models&#x2F;Stable-Diffusion 路径下)，重命名为相同的文件名（即如果你的检查点被命名为768-v-ema.ckpt，配置应被命名为768-v-ema.yaml）,至此就可正常使用2.0模型 (不过个人感觉效果不是很好)</p><p><img src="https://pic2.zhimg.com/v2-014568c4b28e273396c100a2c1236499_b.jpg"></p><h3 id="11-27-deepdanbooru-近日在webui中提供了pytorch实现的版本"><a href="#11-27-deepdanbooru-近日在webui中提供了pytorch实现的版本" class="headerlink" title="11.27 deepdanbooru 近日在webui中提供了pytorch实现的版本"></a>11.27 deepdanbooru 近日在webui中提供了pytorch实现的版本</h3><p>（之前的是由tensorflow实现 所以需要用户机子上同时装pytorch和tensorflow依赖过于庞大）在webui img2img 选项页第一次点击deepdanbooru时程序会自动下载pt检查点文件并调用（速度可能很慢 可以用aria2c 事先下好放到相应路径下 下载地址为 <a href="https://github.com/AUTOMATIC1111/TorchDeepDanbooru/releases/download/v1/model-resnet_custom_v3.pt">https://github.com/AUTOMATIC1111/TorchDeepDanbooru/releases/download/v1/model-resnet_custom_v3.pt</a> ）</p><p><img src="https://pic3.zhimg.com/v2-c67e481ffeccf2bbf7bee77eb26f6d7e_b.jpg"></p><h3 id="11-18日更新-奉上大家期盼已久的-DreamBooth全套教程"><a href="#11-18日更新-奉上大家期盼已久的-DreamBooth全套教程" class="headerlink" title="11.18日更新 奉上大家期盼已久的 DreamBooth全套教程"></a>11.18日更新 奉上大家期盼已久的 DreamBooth全套教程</h3><h3 id="11-11更新-万众期待的dreambooth自训练现在可以在webUI里完成-作者就是曾经提交pr想要将dreambooth-merge到webui主体里的那个大佬-随着webui规模不断庞大-原来的pr合并到主仓库变得不怎么现实-所以作者经过一段时间开发和完善-转而以extension形式提供dreambooth自训练的功能-具体使用将于这两天更新"><a href="#11-11更新-万众期待的dreambooth自训练现在可以在webUI里完成-作者就是曾经提交pr想要将dreambooth-merge到webui主体里的那个大佬-随着webui规模不断庞大-原来的pr合并到主仓库变得不怎么现实-所以作者经过一段时间开发和完善-转而以extension形式提供dreambooth自训练的功能-具体使用将于这两天更新" class="headerlink" title="11.11更新 万众期待的dreambooth自训练现在可以在webUI里完成(作者就是曾经提交pr想要将dreambooth merge到webui主体里的那个大佬,随着webui规模不断庞大,原来的pr合并到主仓库变得不怎么现实,所以作者经过一段时间开发和完善 转而以extension形式提供dreambooth自训练的功能 具体使用将于这两天更新 )"></a>11.11更新 万众期待的dreambooth自训练现在可以在webUI里完成(作者就是曾经提交pr想要将dreambooth merge到webui主体里的那个大佬,随着webui规模不断庞大,原来的pr合并到主仓库变得不怎么现实,所以作者经过一段时间开发和完善 转而以extension形式提供dreambooth自训练的功能 具体使用将于这两天更新 )</h3><p><img src="https://pic2.zhimg.com/v2-d5ea6011b0ff58e5013ecdfcbd533f41_b.jpg"></p><h3 id="11-1更新-庖丁解牛-Stable-diffusion-webui-插件-脚本-依赖-全方位指南"><a href="#11-1更新-庖丁解牛-Stable-diffusion-webui-插件-脚本-依赖-全方位指南" class="headerlink" title="11.1更新 庖丁解牛 Stable-diffusion-webui 插件 脚本 依赖 全方位指南"></a>11.1更新 庖丁解牛 Stable-diffusion-webui 插件 脚本 依赖 全方位指南</h3><h3 id="10-29更新-为防止公网映射服务被滥用、现在在autodl平台使用该服务需先实名-遂推荐SSH端口转发方法部署-操作流程已写更新、可在文章对应位置查看"><a href="#10-29更新-为防止公网映射服务被滥用、现在在autodl平台使用该服务需先实名-遂推荐SSH端口转发方法部署-操作流程已写更新、可在文章对应位置查看" class="headerlink" title="10.29更新 为防止公网映射服务被滥用、现在在autodl平台使用该服务需先实名,遂推荐SSH端口转发方法部署 操作流程已写更新、可在文章对应位置查看"></a>10.29更新 为防止公网映射服务被滥用、现在在autodl平台使用该服务需先实名,遂推荐SSH端口转发方法部署 操作流程已写更新、可在文章对应位置查看</h3><h3 id="10-27更新-中文汉化已上线-webui近期将一些非核心功能转为了插件模块单独解耦-你可以在webui路径下的extension文件夹里下载放置你想使用的任意插件-以下列举一些目前为止比较热门的插件的git项目地址-后续会更新具体使用方法"><a href="#10-27更新-中文汉化已上线-webui近期将一些非核心功能转为了插件模块单独解耦-你可以在webui路径下的extension文件夹里下载放置你想使用的任意插件-以下列举一些目前为止比较热门的插件的git项目地址-后续会更新具体使用方法" class="headerlink" title="10.27更新 中文汉化已上线 webui近期将一些非核心功能转为了插件模块单独解耦,你可以在webui路径下的extension文件夹里下载放置你想使用的任意插件,以下列举一些目前为止比较热门的插件的git项目地址(后续会更新具体使用方法)"></a>10.27更新 中文汉化已上线 webui近期将一些非核心功能转为了插件模块单独解耦,你可以在webui路径下的extension文件夹里下载放置你想使用的任意插件,以下列举一些目前为止比较热门的插件的git项目地址(后续会更新具体使用方法)</h3><ul><li>生成视频 3d、2d 模块 :<a href="https://github.com/deforum-art/deforum-for-automatic1111-webui">https://github.com/deforum-art/deforum-for-automatic1111-webui</a></li><li>美学权重插件(从美学角度优化图片) :<a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients">https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients</a></li><li>历史图片浏览器(查看软件之前生成的图片):<a href="https://github.com/yfszzx/stable-diffusion-webui-images-browser">https://github.com/yfszzx/stable-diffusion-webui-images-browser</a></li></ul><h3 id="10-23更新-webui最近更新了Aesthetic-Gradients-美学权重功能-以插件形式提供-借助该功能-可以在保持作品原始的总体构图上提高美观度。"><a href="#10-23更新-webui最近更新了Aesthetic-Gradients-美学权重功能-以插件形式提供-借助该功能-可以在保持作品原始的总体构图上提高美观度。" class="headerlink" title="10.23更新 webui最近更新了Aesthetic Gradients 美学权重功能(以插件形式提供),借助该功能,可以在保持作品原始的总体构图上提高美观度。"></a>10.23更新 webui最近更新了Aesthetic Gradients 美学权重功能(以插件形式提供),借助该功能,可以在保持作品原始的总体构图上提高美观度。</h3><p>(目前测试下来效果不错,下面是拿昨天训练的废稿做的测试,结合hypernetwork使用效果更佳,支持txt2img和img2img)<a href="https://stable-diffusion-book.vercel.app/5_drawing/#aesthetic-gradients">具体可参阅此链接</a></p><p><img src="https://pic3.zhimg.com/v2-6de0beea600acf0aca69bc5e04eaf61a_b.jpg"></p><p>插件使用界面 (控制参数不变)</p><p><img src="https://pic3.zhimg.com/v2-b152c99b4ee4836c95a8316b2ef4a33e_b.jpg"></p><p>同时开启美学权重和hypernetwork</p><p><img src="https://pic1.zhimg.com/v2-04607b0b575a0ff28e4d1c5a356839fc_b.jpg"></p><p>仅开启美学权重、不用hypernetwork</p><h3 id="10-23-更新-sd在hugging-face更新了1-5版本"><a href="#10-23-更新-sd在hugging-face更新了1-5版本" class="headerlink" title="10.23 更新 sd在hugging face更新了1.5版本"></a>10.23 更新 sd在hugging face更新了1.5版本</h3><h3 id="10-19更新-开个新坑-这几天看到各种prompt里很多都有语法错误-要么就是一堆单词拼错-觉得有必要开个魔法教程-英语教程"><a href="#10-19更新-开个新坑-这几天看到各种prompt里很多都有语法错误-要么就是一堆单词拼错-觉得有必要开个魔法教程-英语教程" class="headerlink" title="10.19更新 开个新坑 这几天看到各种prompt里很多都有语法错误,要么就是一堆单词拼错 觉得有必要开个魔法教程 (英语教程)"></a><strong>10.19更新 开个新坑</strong> 这几天看到各种prompt里很多都有语法错误,要么就是一堆单词拼错 觉得有必要开个魔法教程 (英语教程)</h3><h3 id="10-21-已更新-hypernetwork自训练的详细教程-boss-tensorflow-闪亮登场-ヾ-´-‘-ﾉ"><a href="#10-21-已更新-hypernetwork自训练的详细教程-boss-tensorflow-闪亮登场-ヾ-´-‘-ﾉ" class="headerlink" title="10.21 已更新 hypernetwork自训练的详细教程 (boss tensorflow 闪亮登场) ヾ(*´ ‘*)ﾉ"></a><strong>10.21 已更新 hypernetwork自训练的详细教程 (boss tensorflow 闪亮登场)</strong> ヾ(*´ ‘*)ﾉ</h3><p><img src="https://pic1.zhimg.com/v2-671d018c0dac6fdfc98301d81dbe4518_r.jpg"></p><p>pt训练迭代过程图</p><p><img src="https://pic1.zhimg.com/v2-7ce4992cfbc0b259fbfdfa87bff4a798_r.jpg"></p><p>数据集</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;最近NovalAI的部署教程随处可见,可基本都是windows本地和colab白嫖google羊毛的、更多面向使用者 想要自己在Linux服务器上部署、找了一圈都没满意的、官方的一键脚本没有考虑国内网络的环境、很多大佬也不屑于写诸如此类的东西、导致</summary>
      
    
    
    
    
    <category term="NovelAI" scheme="https://testernan.github.io/tags/NovelAI/"/>
    
    <category term="Stable-Diffusion" scheme="https://testernan.github.io/tags/Stable-Diffusion/"/>
    
    <category term="Colab" scheme="https://testernan.github.io/tags/Colab/"/>
    
  </entry>
  
  <entry>
    <title>test_my_site</title>
    <link href="https://testernan.github.io/2022/06/20/test-my-site/"/>
    <id>https://testernan.github.io/2022/06/20/test-my-site/</id>
    <published>2022-06-20T11:56:30.000Z</published>
    <updated>2022-06-20T11:56:30.365Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://testernan.github.io/2022/06/20/hello-world/"/>
    <id>https://testernan.github.io/2022/06/20/hello-world/</id>
    <published>2022-06-20T11:14:09.405Z</published>
    <updated>2022-06-20T11:14:09.405Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
