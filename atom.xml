<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>TesterNaN的博客</title>
  
  
  <link href="https://testernan.github.io/atom.xml" rel="self"/>
  
  <link href="https://testernan.github.io/"/>
  <updated>2023-03-06T08:47:29.157Z</updated>
  <id>https://testernan.github.io/</id>
  
  <author>
    <name>TesterNaN</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>使用colossalai分布式框架加速你的深度学习训练速度（一）安装与基本构建流程</title>
    <link href="https://testernan.github.io/2023/03/06/%E4%BD%BF%E7%94%A8colossalai%E5%88%86%E5%B8%83%E5%BC%8F%E6%A1%86%E6%9E%B6%E5%8A%A0%E9%80%9F%E4%BD%A0%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83%E9%80%9F%E5%BA%A6%EF%BC%88%E4%B8%80%EF%BC%89%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%9E%84%E5%BB%BA%E6%B5%81%E7%A8%8B/"/>
    <id>https://testernan.github.io/2023/03/06/%E4%BD%BF%E7%94%A8colossalai%E5%88%86%E5%B8%83%E5%BC%8F%E6%A1%86%E6%9E%B6%E5%8A%A0%E9%80%9F%E4%BD%A0%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E8%AE%AD%E7%BB%83%E9%80%9F%E5%BA%A6%EF%BC%88%E4%B8%80%EF%BC%89%E5%AE%89%E8%A3%85%E4%B8%8E%E5%9F%BA%E6%9C%AC%E6%9E%84%E5%BB%BA%E6%B5%81%E7%A8%8B/</id>
    <published>2023-03-06T08:40:01.000Z</published>
    <updated>2023-03-06T08:47:29.157Z</updated>
    
    <content type="html"><![CDATA[<h2 id="前言（废话）"><a href="#前言（废话）" class="headerlink" title="前言（废话）"></a>前言（废话）</h2><p>我们都知道，存在若干种方法可以加速我们在训练深度学习模型时的速度，本人大致知道如下几种（具体）：</p><ul><li>mlkdnn加速</li><li>cuda加速</li><li>JIT加速</li><li>分布式训练</li></ul><p>这些都是成效显著的加速方法，而且属于不同层面的加速。其中基于硬件GPU加速的是目前最主流、效果最好的方案了，在本人自己写框架的时候也发现，深度学习训练过程中也发现，其实可以优化的地方很多：batch size之间可以做并行，单个batch中基于不同的算子实现逻辑，也能做并行；每个训练集的batch之间也能做并行，如何做到高效调度本机GPU的计算资源，或是计算机集群的计算资源，使得整体的并行训练效率尽可能高？</p><p>HPC-AI Technology Inc. 和NUS的研究人员共同开发了“夸父”Colossal-AI，该框架提供了一个并行训练框架来尽可能提升（压榨）你的计算资源的使用效率。并自动实现了一些常用的训练技巧。</p><blockquote><p>由于本人的笔记本前几天又在我relink的时候烂尾了，暂时无法参与本篇文章，所以这篇文章只能展示一下单机单卡的性能压榨，等我把笔记本修好了，再来尝试一下多机多卡分布式训练。</p></blockquote><hr><h2 id="安装、验证"><a href="#安装、验证" class="headerlink" title="安装、验证"></a>安装、验证</h2><h3 id="实验环境"><a href="#实验环境" class="headerlink" title="实验环境"></a>实验环境</h3><table><thead><tr><th>平台\软件包</th><th>版本</th></tr></thead><tbody><tr><td>Win11 WSL2</td><td>0.58.3.0</td></tr><tr><td>NVCC</td><td>11.3</td></tr><tr><td>Python</td><td>3.6.9</td></tr><tr><td>torch</td><td>1.10.2+cu113</td></tr></tbody></table><blockquote><p>GPU为12G的RTX3060</p></blockquote><p>请注意：</p><ol><li>目前colossalai只支持Linux平台下的安装</li><li>尽量让你的NVCC版本&gt;&#x3D;11.3</li></ol><p>不满足第一个条件，则下面pip install时大概率安装失败（今天上午提discussion时开发团队的成员秒回了会在Windows进行试验），不满足第二个条件，有可能失败。</p><blockquote><p>本人主力机就是windows，可喜的是，目前的WSL2已经支持和宿主机共享显卡驱动了，因此，我在WSL2上完成了实验。</p></blockquote><h3 id="安装-colossalai"><a href="#安装-colossalai" class="headerlink" title="安装 colossalai"></a>安装 colossalai</h3><p>一句话就可以安装：</p><p>building wheel的过程很慢，因为会本地编译与CUDA相关的一些程序。</p><p>安装过程可能会出现报错，不过colossalai的setup.py写得很详细了，反正出错了就按照报错信息增加环境变量和安装包就行。</p><p>本人实测下来，你最好在colossalai安装前把tensorboard额外下载好，安装程序对tensorboard版本选取有bug。我在另一台电脑上安装时还遇到了交叉编译警告的问题，添加一个环境变量就好了：</p><pre><code class="bash">$export TORCH_CUDA_ARCH_LIST=&quot;compute capability&quot;</code></pre><h3 id="验证安装是否成功"><a href="#验证安装是否成功" class="headerlink" title="验证安装是否成功"></a>验证安装是否成功</h3><p>我们使用官方给出的例子进行验证，首先创建环境变量DATA，等一下CIFAR-10数据集会下载在DATA代表的文件夹下。</p><p>（更多的例子请看官方的Example仓库 <a href="https://github.com/hpcaitech/ColossalAI-Examples">GitHub - hpcaitech&#x2F;ColossalAI-Examples: Examples of training models with hybrid parallelism using ColossalAI</a>）</p><p>复制下述代码到 eval.py</p><pre><code class="python">from pathlib import Pathfrom colossalai.logging import get_dist_loggerimport colossalaiimport torchimport osfrom colossalai.core import global_context as gpcfrom colossalai.utils import get_dataloaderfrom colossalai.context import Configfrom colossalai.amp import AMP_TYPEfrom torchvision import transformsfrom colossalai.nn.lr_scheduler import CosineAnnealingLRfrom torchvision.datasets import CIFAR10from torchvision.models import resnet34from tqdm import tqdmglobal_config = Config(&#123;    &quot;BATCH_SIZE&quot; : 128,    &quot;NUM_EPOCHS&quot; : 2,    &quot;CONFIG&quot; : &#123;        &quot;fp16&quot; : &#123;            &quot;mode&quot; : AMP_TYPE.TORCH        &#125;    &#125;&#125;)def main():    colossalai.launch_from_torch(config=global_config)    logger = get_dist_logger()    # build resnet    model = resnet34(num_classes=10)    # build dataloaders    print(&quot;loading dataset...&quot;)    train_dataset = CIFAR10(        root=Path(os.environ[&#39;DATA&#39;]),        download=True,        transform=transforms.Compose(            [                transforms.RandomCrop(size=32, padding=4),                transforms.RandomHorizontalFlip(),                transforms.ToTensor(),                transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[                    0.2023, 0.1994, 0.2010]),            ]        )    )    print(&quot;finish loading dataset...&quot;)        test_dataset = CIFAR10(        root=Path(os.environ[&#39;DATA&#39;]),        train=False,        transform=transforms.Compose(            [                transforms.ToTensor(),                transforms.Normalize(mean=[0.4914, 0.4822, 0.4465], std=[                    0.2023, 0.1994, 0.2010]),            ]        )    )    train_dataloader = get_dataloader(dataset=train_dataset,                                      shuffle=True,                                      batch_size=gpc.config.BATCH_SIZE,                                      num_workers=1,                                      pin_memory=True,                                      )    test_dataloader = get_dataloader(dataset=test_dataset,                                     add_sampler=False,                                     batch_size=gpc.config.BATCH_SIZE,                                     num_workers=1,                                     pin_memory=True,                                     )    # build criterion    criterion = torch.nn.CrossEntropyLoss()    # optimizer    optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9, weight_decay=5e-4)    # lr_scheduler    lr_scheduler = CosineAnnealingLR(optimizer, total_steps=gpc.config.NUM_EPOCHS)    engine, train_dataloader, test_dataloader, _ = colossalai.initialize(model,                                                                         optimizer,                                                                         criterion,                                                                         train_dataloader,                                                                         test_dataloader,                                                                         )    for epoch in range(gpc.config.NUM_EPOCHS):        engine.train()        if gpc.get_global_rank() == 0:            train_dl = tqdm(train_dataloader)        else:            train_dl = train_dataloader        for img, label in train_dl:            img = img.cuda()            label = label.cuda()            engine.zero_grad()            output = engine(img)            train_loss = engine.criterion(output, label)            engine.backward(train_loss)            engine.step()        lr_scheduler.step()        engine.eval()        correct = 0        total = 0        for img, label in test_dataloader:            img = img.cuda()            label = label.cuda()            with torch.no_grad():                output = engine(img)                test_loss = engine.criterion(output, label)            pred = torch.argmax(output, dim=-1)            correct += torch.sum(pred == label)            total += img.size(0)        logger.info(            f&quot;Epoch &#123;epoch&#125; - train loss: &#123;train_loss:.5&#125;, test loss: &#123;test_loss:.5&#125;, acc: &#123;correct / total:.5&#125;, lr: &#123;lr_scheduler.get_last_lr()[0]:.5g&#125;&quot;, ranks=[0])if __name__ == &#39;__main__&#39;:    main()</code></pre><p>然后打开命令行，运行：</p><pre><code class="bash">$torchrun --nproc_per_node 1 --master_addr localhost --master_port 8008 eval.py</code></pre><p>由于多机多卡涉及到的通信，所以需要开socket，不过本次是单机，这个参数设了也没啥用，只要确保master_port和本地开启的端口没有冲突就行。</p><p>运行效果：</p><p><img src="https://pic4.zhimg.com/v2-231c65e880949d17ddce911c30e44b0b_b.jpg"></p><p>首次运行需要消耗时间下载CIFAR-10数据集</p><p>运行成功。</p><hr><h2 id="基本使用"><a href="#基本使用" class="headerlink" title="基本使用"></a>基本使用</h2><p>在colossalai的官网上，给出了一套文字教程，个人认为讲得非常清楚：</p><p>使用colossalai框架进行训练的大致流程如下：</p><p><img src="https://pic3.zhimg.com/v2-916e280351363e1d37bc2e5cf0e2fd3a_b.jpg"></p><p>我会大致讲讲使用colossalai构建的每个步骤，这些步骤基本都可以和上述的例子对应起来。所以本人就不再写一个完整的例子了。</p><h3 id="config-py-全局配置"><a href="#config-py-全局配置" class="headerlink" title="config.py 全局配置"></a><a href="http://config.py/">config.py</a> 全局配置</h3><p>colossalai通过一个python文件来作为全局的配置，假设我们把配置文件命名为config.py好了（你当然可以取别的名字），配置的内容可以在后续launch后通过 colossalai.core.global_context 进行访问。</p><p>比如我们的config.py是这样的：</p><p>那么在整个训练系统中，你可以这么访问它：</p><pre><code class="python">import colossalaifrom colossalai.core import global_context as gpccolossalai.launch(config=&#39;./config.py&#39;, ...)# config.py已经被注册成了对象，直接通过属性值访问gpc.config.BATCH_SIZE</code></pre><p>除了基本的训练参数，比如批大小，训练轮数外，配置文件和整个训练系统需要使用的并行加速策略直接相关（说直接点，整个colossalai的并行策略就是在config.py中设置的），这部分配置的名字是固定的。</p><p>我把和并行策略相关的配置属性值列在下表中，附带对应的教程链接，方便查阅：</p><table><thead><tr><th>属性名称</th><th>含义</th><th>链接</th></tr></thead><tbody><tr><td>parallel</td><td>并行配置，是一个字典，可配置的子项为数据并行、流水线并行和序列并行</td><td><a href="https://www.colossalai.org/zh-Hans/docs/basics/configure_parallelization">https://www.colossalai.org/zh-Hans/docs/basics/configure_parallelization</a></td></tr><tr><td>fp16</td><td>混合精度策略</td><td><a href="https://www.colossalai.org/zh-Hans/docs/features/mixed_precision_training">https://www.colossalai.org/zh-Hans/docs/features/mixed_precision_training</a></td></tr><tr><td>gradient_accumulation</td><td>梯度累计次数</td><td><a href="https://www.colossalai.org/zh-Hans/docs/features/gradient_accumulation">https://www.colossalai.org/zh-Hans/docs/features/gradient_accumulation</a></td></tr><tr><td>clip_grad_norm</td><td>梯度裁剪范数</td><td><a href="https://www.colossalai.org/zh-Hans/docs/features/gradient_clipping">https://www.colossalai.org/zh-Hans/docs/features/gradient_clipping</a></td></tr><tr><td>gradient_handler</td><td>自定义处理梯度同步的类</td><td><a href="https://www.colossalai.org/zh-Hans/docs/features/gradient_handler">https://www.colossalai.org/zh-Hans/docs/features/gradient_handler</a></td></tr><tr><td>MOE_MODEL_PARALLEL_SIZE</td><td>一个进程中的混合专家模型数量</td><td><a href="https://www.colossalai.org/zh-Hans/docs/advanced_tutorials/integrate_mixture_of_experts_into_your_model">https://www.colossalai.org/zh-Hans/docs/advanced_tutorials&#x2F;integrate_mixture_of_experts_into_your_model</a></td></tr></tbody></table><p>一种可行的、较为简单的配置如下，很多任务中可以直接复制粘贴：</p><pre><code class="python">from colossalai.amp import AMP_TYPEBATCH_SIZE = 128     # 批次大小NUM_EPOCHS = 10      # 训练10轮fp16 = dict(  mode=AMP_TYPE.TORCH     # AMP后端是pytorh)parallel = dict(          # 并行策略，请注意，pipline的取值和tensor的size的乘积为你GPU的数量（此例中为2 * 4 = 8）    pipeline=2,    tensor=dict(size=4, mode=&#39;2d&#39;))</code></pre><h3 id="colossalai-launch-启动"><a href="#colossalai-launch-启动" class="headerlink" title="colossalai.launch 启动"></a>colossalai.launch 启动</h3><p>通过launch可以将配置文件注入系统中，并初始化各种与网络硬件相关的配置。</p><p>关于分布式训练有几个比较重要的几个概念：</p><ul><li>host: 主训练机的IP</li><li>port: 主训练机的端口</li><li>host: 训练网络中机器的ID</li><li>world size: 网络中机器的数量。</li></ul><p>将这些参数注入系统的方法有很多种，你可以在<a href="https://www.colossalai.org/zh-Hans/docs/basics/launch_colossalai">启动 Colossal-AI | Colossal-AI (colossalai.org)</a> 找到。如果你使用的是版本大于1.10的pytorch，可以使用torch自带的脚本torchrun来启动并输入参数：</p><p>参数如下：</p><ul><li>--nproc_per_node : 每个节点GPU的数量</li><li>--master_addr : 对应上述 host</li><li>--master_port : 对应上述的port</li></ul><p>倘若你的训练脚本为train.py，本地有三块GPU，只用一台机器训练，那么启动训练的脚本为：</p><pre><code class="bash">$torchrun --nproc_per_node 3 --master_addr localhost --master_port 8001 train.py</code></pre><p>上述命令会在该机器上的8001端口开启一个训练服务，如果8001被占用，也可以使用别的端口。</p><h3 id="colossalai-initialize-初始化（模型封装）"><a href="#colossalai-initialize-初始化（模型封装）" class="headerlink" title="colossalai.initialize 初始化（模型封装）"></a>colossalai.initialize 初始化（模型封装）</h3><p>在launch之后，我们就不需要关心系统如何调度计算资源了，我们只需要关心单台机器上如何跑训练代码。</p><p>除此之外，colossalai还做了一件很nice事情，就是将已有的训练代码进行二次封装，无论用户采用什么模型，什么优化器，什么学习率动态更新算法，什么数据加载器，通过colossalai封装后，后续代码都会变得几乎一模一样。</p><p>假设我们已经历经千辛万苦，搞到了训练的几大要素：</p><pre><code class="python"># 后端采用pytorchcolossalai.launch_from_torch(config=&quot;config.py&quot;)# 1. 训练集加载器train_dataloader = MyTrainDataloader()# 2. 测试集加载器test_dataloader = MyTrainDataloader()# 3. 模型model = MyModel()# 4. 优化器optimizer = torch.optim.Adam(model.parameters(), lr=0.001)# 5. 损失函数criterion = torch.nn.CrossEntropyLoss()</code></pre><p>通过colossalai提供的初始化函数可以进行一步封装：</p><pre><code class="python"># 返回四个值： engine对象，训练集加载器，测试集加载器，学习率更新器（这不是必须的，从简，不提）engine, train_dataloader, test_dataloader, _ = colossalai.initialize(                                                                     model,                                                                     optimizer,                                                                     criterion,                                                                     train_dataloader,                                                                     test_dataloader,                                                                    )</code></pre><h3 id="engine封装"><a href="#engine封装" class="headerlink" title="engine封装"></a>engine封装</h3><p>其中的engine就是对模型、优化、损失函数的封装，它同时继承了模型，优化器和损失函数的一堆方法，常用操作我都整理在下表中了：</p><table><thead><tr><th>方法</th><th>解释</th></tr></thead><tbody><tr><td>engine(inputs)</td><td>前向计算，等价于model(inputs)</td></tr><tr><td>engine.zero_grad()</td><td>清空梯度，等价于optimizer.zero_grad()</td></tr><tr><td>engine.step()</td><td>更新参数，等价于optimizer.step()</td></tr><tr><td>engine.criterion(output, label)</td><td>计算损失，等价于criterion(output, label)</td></tr><tr><td>engine.backward(loss)</td><td>反向传播，等价于loss.backward()</td></tr><tr><td>torch.save(engine.model.state_dict(), f&#x3D;…)</td><td>保存模型</td></tr><tr><td>engine.model.load_state_dict(torch.load(f&#x3D;…))</td><td>读取模型</td></tr><tr><td>engine.train()</td><td>训练模式，等价于model.train()</td></tr><tr><td>engine.eval()</td><td>评估模式，等价于model.eval()</td></tr></tbody></table><blockquote><p>请注意，使用engine封装后的模型的state_dict()的每一个key会多出一个”model.”的前缀，它无法直接装载进入一个没有封装成engine的model中，如果你偏要这么做，那么请在load之前对每个key使用lstrip(“model.”)方法进行前缀去除。</p></blockquote><p>其余的步骤和torch常规训练一致。此处就不赘述了。需要注意的是，由于需要使用torchrun脚本进行启动，请不要直接在jupyter notebook中启动训练代码。</p><p>除了engine对象，官方还提供了一个更加高级的封装Trainer，它是对engine的进一步封装。使用Trainer，就能实现Keras风格的直接使用fit函数进行拟合，并且Trainer还提供了使用钩子函数的接口，由于本次实验未使用Trainer，所以留到下一章去讲了，感兴趣的读者可以参考colossalai的官方文档来学习Trainer：<br><a href="https://link.zhihu.com/?target=https://www.colossalai.org/zh-Hans/docs/basics/engine_trainer">如何在训练中使用 Engine 和 Trainer | Colossal-AI</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;前言（废话）&quot;&gt;&lt;a href=&quot;#前言（废话）&quot; class=&quot;headerlink&quot; title=&quot;前言（废话）&quot;&gt;&lt;/a&gt;前言（废话）&lt;/h2&gt;&lt;p&gt;我们都知道，存在若干种方法可以加速我们在训练深度学习模型时的速度，本人大致知道如下几种（具体）：&lt;/p&gt;
&lt;</summary>
      
    
    
    
    
    <category term="ColossalAI" scheme="https://testernan.github.io/tags/ColossalAI/"/>
    
    <category term="分布式训练" scheme="https://testernan.github.io/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"/>
    
  </entry>
  
  <entry>
    <title>使用stable-diffusion-webui部署NovelAi/Stable Diffusion 保姆级教程、命令解释、原理讲解(colab、windows、Linux )</title>
    <link href="https://testernan.github.io/2023/03/06/%E4%BD%BF%E7%94%A8stable-diffusion-webui%E9%83%A8%E7%BD%B2NovelAi-Stable-Diffusion-%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%95%99%E7%A8%8B%E3%80%81%E5%91%BD%E4%BB%A4%E8%A7%A3%E9%87%8A%E3%80%81%E5%8E%9F%E7%90%86%E8%AE%B2%E8%A7%A3-colab%E3%80%81windows%E3%80%81Linux/"/>
    <id>https://testernan.github.io/2023/03/06/%E4%BD%BF%E7%94%A8stable-diffusion-webui%E9%83%A8%E7%BD%B2NovelAi-Stable-Diffusion-%E4%BF%9D%E5%A7%86%E7%BA%A7%E6%95%99%E7%A8%8B%E3%80%81%E5%91%BD%E4%BB%A4%E8%A7%A3%E9%87%8A%E3%80%81%E5%8E%9F%E7%90%86%E8%AE%B2%E8%A7%A3-colab%E3%80%81windows%E3%80%81Linux/</id>
    <published>2023-03-06T08:13:05.000Z</published>
    <updated>2023-03-06T08:19:53.501Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>最近NovalAI的部署教程随处可见,可基本都是windows本地和colab白嫖google羊毛的、更多面向使用者 想要自己在Linux服务器上部署、找了一圈都没满意的、官方的一键脚本没有考虑国内网络的环境、很多大佬也不屑于写诸如此类的东西、导致踩了无数的坑才成功部署 希望借此文来帮助和我一样想部署在Linux服务器上的同学们,也好对AI黑盒里的技术窥视一番  </p></blockquote><ul><li><p>正文开始前先讲一讲、目前为止SD的部署方案汇总</p></li><li><p>colab(科学上网 谷歌账号 包含两种ui界面的colab)</p></li><li><p><a href="https://colab.research.google.com/drive/1_Ma71L6uGbtt6UQyA3FjqW2lcZ5Bjck-">原版ui+naifu</a></p></li><li><p><a href="https://colab.research.google.com/drive/1Y5WX9F69xibL6sJ9crgnm3BgPMpikAsT#scrollTo=ZzRNMT42Gw_p">sd-webui+novelai</a></p></li><li><p><a href="https://breezy-andesaurus-8f0.notion.site/colab-2f1f5b4815e74e34a9add78afdab83cd">个人整理notion简陋教程</a></p></li><li><p>windows(本地)</p></li><li><p><a href="https://b23.tv/q5nTEEG">b站教程</a></p></li><li><p><strong>Linux+SDWebUi 本文内容</strong></p></li><li><p><strong>利用autodl平台自带的公网ip映射服务( 强烈推荐,没有有效时间限制,访问速度快)</strong></p></li><li><p><strong>gradio</strong> (负责自动化生成部署生成一个网页,72小时后失效)</p></li><li><p>Linux 前后端分离 docker nginx postgresql (需要自己的公网ip或域名 显存 内存 要求最高)</p></li></ul><p>10.29号更新:优雅的远程开发工作流</p><h2 id="正文部分-Linux-SDWebUi-部署NovelAi"><a href="#正文部分-Linux-SDWebUi-部署NovelAi" class="headerlink" title="正文部分:Linux+SDWebUi 部署NovelAi"></a>正文部分:Linux+SDWebUi 部署NovelAi</h2><ul><li><p><strong>用到的工具</strong></p></li><li><p>aria2 (类似于curl的下载软件 用于后面下载模型文件)</p></li><li><p><strong>自行安装的依赖(就是把依赖中的硬骨头先一个个手动安装 别的借助requirement.txt自动安装)</strong></p></li><li><p>pytorch cuda</p></li><li><p><strong>11.3号更新 gradio3.8已经可以用pip正常下载</strong></p></li><li><p><strong>用到的开源组件</strong></p></li><li><p>stable-diffusion(sd本体、webUI就是封装了个UI(当然还集成了一众优秀的功能)让我们能通过可视化界面而不是通过命令行参数使用SD绘画创作)</p></li><li><p>BLIP (interrogate CLIP的依赖 负责img2img中描述input图像内容并输入至prompt框)</p></li><li><p>taming-transformers (stablediffusion的高分辨率图像的生成)</p></li><li><p>k-diffusion(为SD提供samplers(采样器)SDE(随机微分方程)和ODE(常微分方程))</p></li><li><p>midas(负责为sd 深度模型提供支持)</p></li></ul><p>更多详情请见</p><h3 id="开始"><a href="#开始" class="headerlink" title="开始"></a>开始</h3><h2 id="TLDR-2023-2-3更新"><a href="#TLDR-2023-2-3更新" class="headerlink" title="TLDR(2023.2.3更新)"></a>TLDR(2023.2.3更新)</h2><p><strong>制作镜像封装了最新的webUI,内置所有依赖所需的权重文件(考虑到webUI在更换依赖到pytorch 1 .13. 1后性能方面提升显著,也将迎来稳定期,所以做一个全面稳定的镜像方便自己和大家使用),内置anything4.5模型、常用vae权重文件、以及一些常用插件</strong></p><p><strong>无需任何编程基础</strong> <a href="https://www.autodl.com/register?code=6dd7e133-fd0f-432a-ad52-d683ada06582">注册autodl账号</a> <strong>后直接照着下面视频使用教程操作即可完成部署使用的全过程</strong></p><p><a href="https://www.codewithgpu.com/i/AUTOMATIC1111/stable-diffusion-webui/stablediffusionwebui">镜像链接</a></p><p>使用教程</p><hr><h2 id="选用autodl平台的服务器-别的服务器平台基本一致"><a href="#选用autodl平台的服务器-别的服务器平台基本一致" class="headerlink" title="选用autodl平台的服务器(别的服务器平台基本一致)"></a><strong>选用autodl平台的服务器(别的服务器平台基本一致)</strong></h2><ul><li><p>注册 <a href="https://www.autodl.com/register?code=6dd7e133-fd0f-432a-ad52-d683ada06582">autodl注册界面(新用户注册会送十块钱)</a></p></li><li><p>选实例 参照colab谷歌给我们白嫖的机器配置 (autodl 性价比推荐 内蒙古 A5000 24G显存 )</p></li><li><p>选镜像 直接miniconda python3.8 cuda11.3</p></li><li><p>(2023.1.25 更新 已封装了webUI全套镜像内置所有依赖所需的权重文件 内置anything4.5 复制镜像创建实例后 直接在webUI目录下执行启动命令即可启动镜像cuda11.7 pytorch 1.13.0)</p></li><li><p>点击我的实例-快捷工具-jupyterLab 启动页点击终端新建一个终端</p></li></ul><h2 id="下载安装依赖"><a href="#下载安装依赖" class="headerlink" title="下载安装依赖"></a><strong>下载安装依赖</strong></h2><ul><li><p>进入数据盘路径 <code>cd autodl-tmp</code></p></li><li><p><a href="https://www.autodl.com/docs/network_turbo/">加速github下载速度</a>(找到自己的区并在终端输入相应的语句 )</p></li><li><p>更新下apt下载工具</p></li><li><p><code>apt update</code></p></li><li><p>补充安装python3环境</p></li><li><p><code>sudo apt install wget git python3 python3-venv</code></p></li><li><p>用pip安装 pytorch 和cuda (服务器conda有bug pip默认就加过速了 所以直接用pip下载安装)</p></li><li><p><code>pip3 install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu113</code></p></li><li><p>检查cuda和pytorch是否可用</p></li><li><p><code>python -c &quot;import torch; print(torch.cuda.is_available())&quot;</code></p></li><li><p>下载aria2 类似于curl 下载软件 用于后面下载gradio和模型文件</p></li><li><p><code>apt install aria2</code></p></li><li><p><strong>11</strong> <strong>.3 号更新 gradio更新到 3.8 版本 现在可以直接通过pip 从远处包仓库中下载安装 pip install gradio&#x3D;&#x3D;3.8即可</strong></p></li><li><p>gradio (负责自动化生成部署生成一个网页(前端和中间件去代理绘图程序API),使你能通过http和前端界面与后端API交互)</p></li><li><p>11 .27 号更新 webui又加了一堆依赖… 其中torchsde会出现pip下载过程镜像源中找不到对应包的问题 torchsde是webui近期更新后增加的依赖 可以去pypi 上搜torchsde 然后将安装包whl文件下到机子上 然后pip install 加刚才的whl文件名 手动安装</p></li></ul><h3 id="下载Stable-Diffusion-Webui框架和SD绘画各个步骤用到的模型框架"><a href="#下载Stable-Diffusion-Webui框架和SD绘画各个步骤用到的模型框架" class="headerlink" title="下载Stable-Diffusion-Webui框架和SD绘画各个步骤用到的模型框架"></a><strong>下载Stable-Diffusion-Webui框架和SD绘画各个步骤用到的模型框架</strong></h3><p>Stable Diffusion WebUi简称 SDWebUi，web UI是一个基于 Gradio 库的 Stable Diffusion 浏览器界面。</p><ul><li><p>下载 web ui 框架并进入路径</p></li><li><p><code>git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git</code></p></li><li><p><code>cd stable-diffusion-webui</code></p></li><li><p>下载 StableDiffusion(绘图) 和 CodeFormr(脸部修复)</p></li><li><p><code>mkdir repositories</code></p></li><li><p><code>git clone https://github.com/CompVis/stable-diffusion.git repositories/stable-diffusion</code></p></li><li><p><code>git clone https://github.com/CompVis/taming-transformers.git repositories/taming-transformers</code></p></li><li><p><code>git clone https://github.com/sczhou/CodeFormer.git repositories/CodeFormer</code></p></li><li><p><code>git clone https://github.com/salesforce/BLIP.git repositories/BLIP</code></p></li><li><p>下载安装Stable Diffusion的依赖</p></li><li><p><code>pip install transformers==4.19.2 diffusers invisible-watermark --prefer-binary</code></p></li><li><p>下载安装k-diffusion</p></li><li><p><code>pip install git+https://github.com/crowsonkb/k-diffusion.git --prefer-binary</code></p></li><li><p>下载gfpgan 可选 负责脸部修复face restoration</p></li><li><p><code>pip install gfpgan</code></p></li><li><p>安装之前下载的CodeFormer的依赖</p></li><li><p><code>pip install -r repositories/CodeFormer/requirements.txt --prefer-binary</code></p></li><li><p>安装 web ui框架的依赖</p></li><li><p><code>pip install -r requirements.txt --prefer-binary</code></p></li><li><p>更新numpy到最新版本(因为之前很多包都会引用到<code>numpy</code>所以为了保证版本统一,在此更新为最新版)</p></li><li><p><code>pip install -U numpy --prefer-binary</code></p></li></ul><h2 id="下载模型文件"><a href="#下载模型文件" class="headerlink" title="下载模型文件"></a><strong>下载模型文件</strong></h2><p>如需别的模型请自行替换(也可以多个并存,根据需要在画图过程中在页面左上角切换) 位置一律放在&#x2F;stable-diffusion-webui&#x2F;models&#x2F;Stable-diffusion&#x2F; 下</p><p>10.19号成功解决模型切换的问题后、现在可以控制相同prompt seed,快速对照SDv1.4和NovelAI生成效果.</p><p><img src="https://pic1.zhimg.com/v2-d1a5c604913f3fc4da39dba5804bc1f4_r.jpg"></p><p>Stable Diffusion1.4与NovelAI对照效果</p><h3 id="NovelAI-10-20号更新-因为链接都有期限-大家最好自己备份下-更多模型可见"><a href="#NovelAI-10-20号更新-因为链接都有期限-大家最好自己备份下-更多模型可见" class="headerlink" title="NovelAI(10.20号更新 因为链接都有期限 大家最好自己备份下 ) 更多模型可见"></a>NovelAI(10.20号更新 因为链接都有期限 大家最好自己备份下 ) 更多模型可见</h3><p><a href="https://rentry.org/sdmodels#">https://rentry.org/sdmodels#</a></p><pre><code class="bash">aria2c --summary-interval=10 -d ./models/Stable-diffusion/ -x 3 --allow-overwrite=true https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/animefull-latest.tar</code></pre><p>因为这个链接下的是tar压缩包 所以需要tar xf 解压 并rm 删除压缩包 (具体命令自行网上查阅)<strong>注意</strong>: 最后ckpt一定要在 <strong>&#x2F;models&#x2F;Stable-diffusion&#x2F;目录下</strong></p><ul><li>tar xf animefull-latest.tar</li><li>rm animefull-latest.tar</li></ul><h3 id="Stable-diffusion-1-4"><a href="#Stable-diffusion-1-4" class="headerlink" title="Stable-diffusion 1.4"></a>Stable-diffusion 1.4</h3><p>去HuggingFace去下载(注册账号 同意条款 然后就能看到下载链接)</p><p><a href="https://huggingface.co/CompVis/stable-diffusion-v-1-4-original">CompVis&#x2F;stable-diffusion-v-1-4-original · Hugging Face</a></p><h3 id="10-23日更新-Stable-diffusion-1-5"><a href="#10-23日更新-Stable-diffusion-1-5" class="headerlink" title="(10.23日更新)Stable-diffusion 1.5"></a>(10.23日更新)Stable-diffusion 1.5</h3><p><a href="https://huggingface.co/runwayml/stable-diffusion-v1-5">runwayml&#x2F;stable-diffusion-v1-5 · Hugging Face</a></p><h2 id="运行"><a href="#运行" class="headerlink" title="运行"></a><strong>运行</strong></h2><p>下面是运行程序的两种方法 <strong>(10.18日更新 :添加一种更好的方法,即利用autodl平台的公网ip映射服务来实现部署服务,无需借助gradio,强烈推荐)</strong></p><hr><h3 id="方法一-利用autodl平台自带的公网ip映射服务-没有有效时间限制-访问速度快"><a href="#方法一-利用autodl平台自带的公网ip映射服务-没有有效时间限制-访问速度快" class="headerlink" title="方法一:利用autodl平台自带的公网ip映射服务(没有有效时间限制,访问速度快)"></a>方法一:利用autodl平台自带的公网ip映射服务(没有有效时间限制,访问速度快)</h3><p><strong>10.28号更新、为防止该服务被滥用、现在在autodl平台使用该服务需先实名制、所以使用时请合法合规</strong></p><p>AutoDL为每个实例都预留了一个可对外暴露的端口，技术实现为将实例中的<code>6006</code>端口映射到公网可供访问的<code>ip:port</code>上,如果您自启动的服务带有Web网页那么点击访问后就可以打开Web网页。如果您启动的是API服务，那么通过打开的地址进行API调用即可（因为没有Web网页，打开后不会有任何网页显示）</p><ul><li>1 在终端中输入运行下列命令–port 6006将程序部署在服务器本机的6006端口</li></ul><pre><code class="bash">COMMANDLINE_ARGS=&quot;--medvram --always-batch-cond-uncond --port 6006&quot; REQS_FILE=&quot;requirements.txt&quot; python launch.py</code></pre><ul><li>2 找到控制台界面,找到你实例的自定义服务按钮,点击访问即可进入autodl平台帮我们映射到公网的网站(之后可以从其他设备的浏览器访问这个网址来使用部署的绘画服务)</li></ul><p><img src="https://pic3.zhimg.com/v2-cf2a56fad275dd49a0eaef0930277dca_b.jpg"></p><ul><li>如需了解更多,可在autodl帮助文档的这个子页面中找到 <a href="https://www.autodl.com/docs/port/">https://www.autodl.com/docs/port/</a></li></ul><h3 id="方法二-SSH端口转发-ssh隧道-10-28号autodl-公网映射监管收紧后-推荐该方法-安全系数最高"><a href="#方法二-SSH端口转发-ssh隧道-10-28号autodl-公网映射监管收紧后-推荐该方法-安全系数最高" class="headerlink" title="方法二:SSH端口转发(ssh隧道) 10.28号autodl 公网映射监管收紧后 推荐该方法 安全系数最高"></a>方法二:SSH端口转发(ssh隧道) 10.28号autodl 公网映射监管收紧后 推荐该方法 安全系数最高</h3><p>访问一个远程机器上没有公开的端口。这种情况下可以使用SSH隧道技术(或者叫端口转发)将所需的远程端口 “转发 “到你的本地机器,然后就可以通过访问自己机器的本地端口来使用远程程序的相应端口上的服务</p><p>限于篇幅控制、具体操作见下文中的具体演示</p><p><img src="https://pic1.zhimg.com/v2-89575d9803519dba7b28b276fc790ab0_b.jpg"></p><p>演示</p><h3 id="方法三-利用gradio提供部署服务-每次可使用72小时-相较于第一种方法访问速度会慢些-安全性差"><a href="#方法三-利用gradio提供部署服务-每次可使用72小时-相较于第一种方法访问速度会慢些-安全性差" class="headerlink" title="方法三:利用gradio提供部署服务(每次可使用72小时,相较于第一种方法访问速度会慢些,安全性差)"></a>方法三:利用gradio提供部署服务(每次可使用72小时,相较于第一种方法访问速度会慢些,安全性差)</h3><p>如果你用的不是autodl平台,别的平台估计也有类似的公网ip映射服务 (我猜的2333) 如果没有而又想要通过URL给他人使用演示,就只能借助gradio这种方法了</p><ol><li>在终端中输入运行下列命令</li></ol><p><code>COMMANDLINE_ARGS=&quot;--medvram --always-batch-cond-uncond --share&quot; REQS_FILE=&quot;requirements.txt&quot; python launch.py</code></p><p>--share参数 会得到一个以.app.gradio 结尾的链接，这是在协作中使用该程序的预期方式。(不加 –share 没法远程使用)</p><p>2. 访问上面的链接即可开始使用</p><h3 id="运行程序的一些命令行参数及解释"><a href="#运行程序的一些命令行参数及解释" class="headerlink" title="运行程序的一些命令行参数及解释"></a>运行程序的一些命令行参数及解释</h3><table><thead><tr><th>命令行参数</th><th>解释</th></tr></thead><tbody><tr><td>--xformers</td><td>使用xformers库。极大地改善了内存消耗和速度。Windows 版本安装由C43H66N12O12S2 维护的二进制文件</td></tr><tr><td>--force-enable-xformers</td><td>无论程序是否认为您可以运行它，都启用 xformers。不要报告你运行它的错误。</td></tr><tr><td>--opt-split-attention</td><td>Cross attention layer optimization 优化显着减少了内存使用，几乎没有成本（一些报告改进了性能）。黑魔法。默认情况下torch.cuda，包括 NVidia 和 AMD 卡。</td></tr><tr><td>--disable-opt-split-attention</td><td>禁用上面的优化</td></tr><tr><td>--opt-split-attention-v1</td><td>使用上述优化的旧版本，它不会占用大量内存（它将使用更少的 VRAM，但会限制您可以制作的最大图片大小）。</td></tr><tr><td>--medvram</td><td>通过将稳定扩散模型分为三部分，使其消耗更少的VRAM，即cond（用于将文本转换为数字表示）、first_stage（用于将图片转换为潜在空间并返回）和unet（用于潜在空间的实际去噪），并使其始终只有一个在VRAM中，将其他部分发送到CPU RAM。降低性能，但只会降低一点-除非启用实时预览。</td></tr><tr><td>--lowvram</td><td>对上面更彻底的优化，将 unet 拆分成多个模块，VRAM 中只保留一个模块,破坏性能</td></tr><tr><td>*do-not-batch-cond-uncond</td><td>防止在采样过程中对正面和负面提示进行批处理，这基本上可以让您以 0.5 批量大小运行，从而节省大量内存。降低性能。不是命令行选项，而是使用–medvramor 隐式启用的优化–lowvram。</td></tr><tr><td>--always-batch-cond-uncond</td><td>禁用上述优化。只有与–medvram或–lowvram一起使用才有意义</td></tr><tr><td>--opt-channelslast</td><td>更改 torch 内存类型，以稳定扩散到最后一个通道,效果没有仔细研究。</td></tr></tbody></table><h2 id="Q-amp-A-自己碰到的bug-欢迎评论区添加"><a href="#Q-amp-A-自己碰到的bug-欢迎评论区添加" class="headerlink" title="Q&amp;A(自己碰到的bug 欢迎评论区添加)"></a><strong>Q&amp;A(自己碰到的bug 欢迎评论区添加)</strong></h2><ul><li><p>Q：torchsde报错找不到合适版本</p></li><li><p>A：torchsde是webui近期更新后增加的依赖 可以去pypi 上搜torchsde 然后将安装包whl文件下到机子上 然后pip install 加刚才的whl文件名 手动安装</p></li><li><p>Q:系统环境混乱 重置</p></li><li><p>A:关机更换镜像 再开机</p></li><li><p>Q:报错未知参数 scale</p></li><li><p>A:gradio更新到最新3.5</p></li><li><p>Q:出现 <code>ERROR: pip&#39;s dependency resolver does not currently take into account all the packages that are installed.</code>报错</p></li><li><p>A:不用管,倒数第三步解决该问题</p></li><li><p>Q:换别的模型报错(<strong>10.19号更新)</strong></p></li><li><p>A:10.19号更新 确定为之前版本Stable Diffusion Webui的bug <strong>在Stable Diffusion Webui 路径下 执行 git pull 命令拉取最新版webui 即可解决(估计是10.18号的官方更新代码中修复的)</strong></p></li><li><p>Q:每次重启机器后如何再次运行程序</p></li><li><p>A:cd 到webui路径下,输入最后那串命令(github加速 export 环境变量在当前终端关闭后会失效 所以重启后需再次执行)</p></li><li><p>Q:autodl系统盘占用特别多,不知道怎么回事就满了</p></li><li><p>A:</p></li><li><p>Q:export 环境变量加速github下载后,本地端口转发 gradio安全性报错</p></li></ul><p><img src="https://pic3.zhimg.com/v2-e627c7bc7090443b4675c024193e0a9e_b.png"></p><ul><li>A:到gradio源码里把检查判定逻辑那几行代码注释掉(每次git pull 更新webui前记得把注释取消)</li></ul><p><img src="https://pic3.zhimg.com/v2-b7b9f815d3743c5e30586f3e3d9de236_b.jpg"></p><h2 id="参考链接"><a href="#参考链接" class="headerlink" title="参考链接"></a><strong>参考链接</strong></h2><p><a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki">SDwebui官方文档</a></p><p><a href="https://www.autodl.com/docs/deps/">autodl帮助文档</a></p><p><a href="https://stable-diffusion-book.vercel.app/">stable-diffusion-book</a></p><hr><p>ps: 是的,封面就是用部署完的服务器画的 ╰( ’ ’ )╮</p><h2 id="更新动态-后续会按技术原理部署、使用训练、新闻动态等分类整理重构下-便于大家阅读索引"><a href="#更新动态-后续会按技术原理部署、使用训练、新闻动态等分类整理重构下-便于大家阅读索引" class="headerlink" title="更新动态(后续会按技术原理部署、使用训练、新闻动态等分类整理重构下,便于大家阅读索引)"></a>更新动态(后续会按技术原理部署、使用训练、新闻动态等分类整理重构下,便于大家阅读索引)</h2><h3 id="2023-1-25-最新的webUI-又更新了环境需求-要pytorch-1-13-1-pip镜像源里尚无-需要自行去pypi下载"><a href="#2023-1-25-最新的webUI-又更新了环境需求-要pytorch-1-13-1-pip镜像源里尚无-需要自行去pypi下载" class="headerlink" title="2023.1.25 最新的webUI 又更新了环境需求 要pytorch 1.13.1 pip镜像源里尚无 需要自行去pypi下载"></a>2023.1.25 最新的webUI 又更新了环境需求 要pytorch 1.13.1 pip镜像源里尚无 需要自行去pypi下载</h3><p>为方便大家更新 已全套打包上传<a href="https://pan.baidu.com/s/1fMoGBYG3_LND5A3s6S9WKQ?pwd=kw2j">百度云</a> cp38 linux 和我环境一致可以直接下载安装</p><ul><li>先卸载 这三个</li><li>然后在pip install 加文件名 本地安装</li></ul><p><img src="https://pic3.zhimg.com/v2-0478db1797e9e10ea1314fd5abc3aaf6_b.jpg"></p><h3 id="2023-1-9-新年第一篇-讲讲如何找到心仪的模型"><a href="#2023-1-9-新年第一篇-讲讲如何找到心仪的模型" class="headerlink" title="2023.1.9 新年第一篇,讲讲如何找到心仪的模型"></a>2023.1.9 新年第一篇,讲讲如何找到心仪的模型</h3><h3 id="11-27-stable-diffusion-2-0近日已发布-因为使用方法与之前略有不同-所以介绍一下"><a href="#11-27-stable-diffusion-2-0近日已发布-因为使用方法与之前略有不同-所以介绍一下" class="headerlink" title="11.27 stable diffusion 2.0近日已发布 因为使用方法与之前略有不同 所以介绍一下"></a>11.27 stable diffusion 2.0近日已发布 因为使用方法与之前略有不同 所以介绍一下</h3><ol><li>下载模型文件 768-v-ema.ckpt</li></ol><p><a href="https://huggingface.co/stabilityai/stable-diffusion-2/tree/main">stabilityai&#x2F;stable-diffusion-2 at main</a></p><p><img src="https://pic2.zhimg.com/v2-56853fa21324bc8bccbcfc3a82539325_b.jpg"></p><p>2.把ckpt文件放入 models&#x2F;Stable-Diffusion 路径下</p><p>3.下载SD2.0模型配置文件 <a href="https://github.com/Stability-AI/stablediffusion/tree/main/configs/stable-diffusion">stablediffusion&#x2F;configs&#x2F;stable-diffusion at main · Stability-AI&#x2F;stablediffusion</a> 即下载下图中的v2-inference-v.yaml，并将其放在与检查点相同的地方(models&#x2F;Stable-Diffusion 路径下)，重命名为相同的文件名（即如果你的检查点被命名为768-v-ema.ckpt，配置应被命名为768-v-ema.yaml）,至此就可正常使用2.0模型 (不过个人感觉效果不是很好)</p><p><img src="https://pic2.zhimg.com/v2-014568c4b28e273396c100a2c1236499_b.jpg"></p><h3 id="11-27-deepdanbooru-近日在webui中提供了pytorch实现的版本"><a href="#11-27-deepdanbooru-近日在webui中提供了pytorch实现的版本" class="headerlink" title="11.27 deepdanbooru 近日在webui中提供了pytorch实现的版本"></a>11.27 deepdanbooru 近日在webui中提供了pytorch实现的版本</h3><p>（之前的是由tensorflow实现 所以需要用户机子上同时装pytorch和tensorflow依赖过于庞大）在webui img2img 选项页第一次点击deepdanbooru时程序会自动下载pt检查点文件并调用（速度可能很慢 可以用aria2c 事先下好放到相应路径下 下载地址为 <a href="https://github.com/AUTOMATIC1111/TorchDeepDanbooru/releases/download/v1/model-resnet_custom_v3.pt">https://github.com/AUTOMATIC1111/TorchDeepDanbooru/releases/download/v1/model-resnet_custom_v3.pt</a> ）</p><p><img src="https://pic3.zhimg.com/v2-c67e481ffeccf2bbf7bee77eb26f6d7e_b.jpg"></p><h3 id="11-18日更新-奉上大家期盼已久的-DreamBooth全套教程"><a href="#11-18日更新-奉上大家期盼已久的-DreamBooth全套教程" class="headerlink" title="11.18日更新 奉上大家期盼已久的 DreamBooth全套教程"></a>11.18日更新 奉上大家期盼已久的 DreamBooth全套教程</h3><h3 id="11-11更新-万众期待的dreambooth自训练现在可以在webUI里完成-作者就是曾经提交pr想要将dreambooth-merge到webui主体里的那个大佬-随着webui规模不断庞大-原来的pr合并到主仓库变得不怎么现实-所以作者经过一段时间开发和完善-转而以extension形式提供dreambooth自训练的功能-具体使用将于这两天更新"><a href="#11-11更新-万众期待的dreambooth自训练现在可以在webUI里完成-作者就是曾经提交pr想要将dreambooth-merge到webui主体里的那个大佬-随着webui规模不断庞大-原来的pr合并到主仓库变得不怎么现实-所以作者经过一段时间开发和完善-转而以extension形式提供dreambooth自训练的功能-具体使用将于这两天更新" class="headerlink" title="11.11更新 万众期待的dreambooth自训练现在可以在webUI里完成(作者就是曾经提交pr想要将dreambooth merge到webui主体里的那个大佬,随着webui规模不断庞大,原来的pr合并到主仓库变得不怎么现实,所以作者经过一段时间开发和完善 转而以extension形式提供dreambooth自训练的功能 具体使用将于这两天更新 )"></a>11.11更新 万众期待的dreambooth自训练现在可以在webUI里完成(作者就是曾经提交pr想要将dreambooth merge到webui主体里的那个大佬,随着webui规模不断庞大,原来的pr合并到主仓库变得不怎么现实,所以作者经过一段时间开发和完善 转而以extension形式提供dreambooth自训练的功能 具体使用将于这两天更新 )</h3><p><img src="https://pic2.zhimg.com/v2-d5ea6011b0ff58e5013ecdfcbd533f41_b.jpg"></p><h3 id="11-1更新-庖丁解牛-Stable-diffusion-webui-插件-脚本-依赖-全方位指南"><a href="#11-1更新-庖丁解牛-Stable-diffusion-webui-插件-脚本-依赖-全方位指南" class="headerlink" title="11.1更新 庖丁解牛 Stable-diffusion-webui 插件 脚本 依赖 全方位指南"></a>11.1更新 庖丁解牛 Stable-diffusion-webui 插件 脚本 依赖 全方位指南</h3><h3 id="10-29更新-为防止公网映射服务被滥用、现在在autodl平台使用该服务需先实名-遂推荐SSH端口转发方法部署-操作流程已写更新、可在文章对应位置查看"><a href="#10-29更新-为防止公网映射服务被滥用、现在在autodl平台使用该服务需先实名-遂推荐SSH端口转发方法部署-操作流程已写更新、可在文章对应位置查看" class="headerlink" title="10.29更新 为防止公网映射服务被滥用、现在在autodl平台使用该服务需先实名,遂推荐SSH端口转发方法部署 操作流程已写更新、可在文章对应位置查看"></a>10.29更新 为防止公网映射服务被滥用、现在在autodl平台使用该服务需先实名,遂推荐SSH端口转发方法部署 操作流程已写更新、可在文章对应位置查看</h3><h3 id="10-27更新-中文汉化已上线-webui近期将一些非核心功能转为了插件模块单独解耦-你可以在webui路径下的extension文件夹里下载放置你想使用的任意插件-以下列举一些目前为止比较热门的插件的git项目地址-后续会更新具体使用方法"><a href="#10-27更新-中文汉化已上线-webui近期将一些非核心功能转为了插件模块单独解耦-你可以在webui路径下的extension文件夹里下载放置你想使用的任意插件-以下列举一些目前为止比较热门的插件的git项目地址-后续会更新具体使用方法" class="headerlink" title="10.27更新 中文汉化已上线 webui近期将一些非核心功能转为了插件模块单独解耦,你可以在webui路径下的extension文件夹里下载放置你想使用的任意插件,以下列举一些目前为止比较热门的插件的git项目地址(后续会更新具体使用方法)"></a>10.27更新 中文汉化已上线 webui近期将一些非核心功能转为了插件模块单独解耦,你可以在webui路径下的extension文件夹里下载放置你想使用的任意插件,以下列举一些目前为止比较热门的插件的git项目地址(后续会更新具体使用方法)</h3><ul><li>生成视频 3d、2d 模块 :<a href="https://github.com/deforum-art/deforum-for-automatic1111-webui">https://github.com/deforum-art/deforum-for-automatic1111-webui</a></li><li>美学权重插件(从美学角度优化图片) :<a href="https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients">https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients</a></li><li>历史图片浏览器(查看软件之前生成的图片):<a href="https://github.com/yfszzx/stable-diffusion-webui-images-browser">https://github.com/yfszzx/stable-diffusion-webui-images-browser</a></li></ul><h3 id="10-23更新-webui最近更新了Aesthetic-Gradients-美学权重功能-以插件形式提供-借助该功能-可以在保持作品原始的总体构图上提高美观度。"><a href="#10-23更新-webui最近更新了Aesthetic-Gradients-美学权重功能-以插件形式提供-借助该功能-可以在保持作品原始的总体构图上提高美观度。" class="headerlink" title="10.23更新 webui最近更新了Aesthetic Gradients 美学权重功能(以插件形式提供),借助该功能,可以在保持作品原始的总体构图上提高美观度。"></a>10.23更新 webui最近更新了Aesthetic Gradients 美学权重功能(以插件形式提供),借助该功能,可以在保持作品原始的总体构图上提高美观度。</h3><p>(目前测试下来效果不错,下面是拿昨天训练的废稿做的测试,结合hypernetwork使用效果更佳,支持txt2img和img2img)<a href="https://stable-diffusion-book.vercel.app/5_drawing/#aesthetic-gradients">具体可参阅此链接</a></p><p><img src="https://pic3.zhimg.com/v2-6de0beea600acf0aca69bc5e04eaf61a_b.jpg"></p><p>插件使用界面 (控制参数不变)</p><p><img src="https://pic3.zhimg.com/v2-b152c99b4ee4836c95a8316b2ef4a33e_b.jpg"></p><p>同时开启美学权重和hypernetwork</p><p><img src="https://pic1.zhimg.com/v2-04607b0b575a0ff28e4d1c5a356839fc_b.jpg"></p><p>仅开启美学权重、不用hypernetwork</p><h3 id="10-23-更新-sd在hugging-face更新了1-5版本"><a href="#10-23-更新-sd在hugging-face更新了1-5版本" class="headerlink" title="10.23 更新 sd在hugging face更新了1.5版本"></a>10.23 更新 sd在hugging face更新了1.5版本</h3><h3 id="10-19更新-开个新坑-这几天看到各种prompt里很多都有语法错误-要么就是一堆单词拼错-觉得有必要开个魔法教程-英语教程"><a href="#10-19更新-开个新坑-这几天看到各种prompt里很多都有语法错误-要么就是一堆单词拼错-觉得有必要开个魔法教程-英语教程" class="headerlink" title="10.19更新 开个新坑 这几天看到各种prompt里很多都有语法错误,要么就是一堆单词拼错 觉得有必要开个魔法教程 (英语教程)"></a><strong>10.19更新 开个新坑</strong> 这几天看到各种prompt里很多都有语法错误,要么就是一堆单词拼错 觉得有必要开个魔法教程 (英语教程)</h3><h3 id="10-21-已更新-hypernetwork自训练的详细教程-boss-tensorflow-闪亮登场-ヾ-´-‘-ﾉ"><a href="#10-21-已更新-hypernetwork自训练的详细教程-boss-tensorflow-闪亮登场-ヾ-´-‘-ﾉ" class="headerlink" title="10.21 已更新 hypernetwork自训练的详细教程 (boss tensorflow 闪亮登场) ヾ(*´ ‘*)ﾉ"></a><strong>10.21 已更新 hypernetwork自训练的详细教程 (boss tensorflow 闪亮登场)</strong> ヾ(*´ ‘*)ﾉ</h3><p><img src="https://pic1.zhimg.com/v2-671d018c0dac6fdfc98301d81dbe4518_r.jpg"></p><p>pt训练迭代过程图</p><p><img src="https://pic1.zhimg.com/v2-7ce4992cfbc0b259fbfdfa87bff4a798_r.jpg"></p><p>数据集</p>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;最近NovalAI的部署教程随处可见,可基本都是windows本地和colab白嫖google羊毛的、更多面向使用者 想要自己在Linux服务器上部署、找了一圈都没满意的、官方的一键脚本没有考虑国内网络的环境、很多大佬也不屑于写诸如此类的东西、导致</summary>
      
    
    
    
    
    <category term="NovelAI" scheme="https://testernan.github.io/tags/NovelAI/"/>
    
    <category term="Stable-Diffusion" scheme="https://testernan.github.io/tags/Stable-Diffusion/"/>
    
    <category term="Colab" scheme="https://testernan.github.io/tags/Colab/"/>
    
  </entry>
  
  <entry>
    <title>test_my_site</title>
    <link href="https://testernan.github.io/2022/06/20/test-my-site/"/>
    <id>https://testernan.github.io/2022/06/20/test-my-site/</id>
    <published>2022-06-20T11:56:30.000Z</published>
    <updated>2022-06-20T11:56:30.365Z</updated>
    
    
    
    
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://testernan.github.io/2022/06/20/hello-world/"/>
    <id>https://testernan.github.io/2022/06/20/hello-world/</id>
    <published>2022-06-20T11:14:09.405Z</published>
    <updated>2022-06-20T11:14:09.405Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre><code class="bash">$ hexo new &quot;My New Post&quot;</code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre><code class="bash">$ hexo server</code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre><code class="bash">$ hexo generate</code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre><code class="bash">$ hexo deploy</code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.io/docs/&quot;&gt;documentation&lt;/a&gt; for</summary>
      
    
    
    
    
  </entry>
  
</feed>
